### Author: Heli Qi
### Affiliation: NAIST
### Date: 2022.09
# This is a template of data loading configuration on the train-clean-100 dataset of the LibriSpeech corpus
# This configuration fits 2 Ã— Geforce 1080 Ti GPUs with 11GB memory.
# You can modify this configuration according to your computational resource, such as batch_len and ngpu.
# However, you may not be able to reproduce exactly the same performance with different GPU setting.


### --- Reference Zone --- ###
# data-related
dataset: librispeech
train_set: train-clean-100
valid_set: dev-clean

wav_format: wav
txt_format: librispeech
data_root: !ref datasets/<dataset>/data/<wav_format>

# batch-related
batch_len: 1.2e7
### ---------------------- ###


### --- Training Iterator Configuration --- ###
train:
    type: block.BlockIterator
    conf:
        dataset_type: speech_text.SpeechTextDataset
        dataset_conf:
            main_data:
                feat: !ref <data_root>/<train_set>/idx2wav
                text: !ref <data_root>/<train_set>/idx2<txt_format>_text

        data_len: !ref <data_root>/<train_set>/idx2wav_len
        shuffle: True
        is_descending: True
        batch_len: !ref <batch_len>

### --- Validation Iterator Configuration --- ###
valid:
    type: abs.Iterator
    conf:
        dataset_type: speech_text.SpeechTextDataset
        dataset_conf:
            main_data:
                feat: !ref <data_root>/<valid_set>/idx2wav
                text: !ref <data_root>/<valid_set>/idx2<txt_format>_text

        shuffle: False
        data_len: !ref <data_root>/<valid_set>/idx2wav_len

